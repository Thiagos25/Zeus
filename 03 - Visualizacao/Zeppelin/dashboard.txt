%sql
select data, nome_usuario, nome_dispositivo, avg(potencia_gasta) as potencia_gasta, avg(mintemp) as mintemp, avg(maxtemp) as maxtemp, avg(avgtemp) as avgtemp, avg(totalprecip_mm) as totalprecip_mm 
    from (
        select Clima.datatemp as data, Usuario.nome_usuario, Dispositivo.nome as nome_dispositivo, avg(Energia.potencia) as potencia_gasta, avg(Clima.mintemp) as mintemp, avg(Clima.maxtemp) as maxtemp, avg(Clima.avgtemp) as avgtemp, avg(Clima.totalprecip_mm) as totalprecip_mm 
        from Zeus.Energia
        left outer join Zeus.Usuario on Energia.user_id = Usuario.id
        left outer join Zeus.Dispositivo on Energia.dispositivo_id = Dispositivo.id
        left outer join Zeus.Cidade on Dispositivo.id_cidade = Cidade.id
        left outer join Zeus.Clima on Cidade.id = Clima.id_cidade
        Group by Clima.datatemp, Usuario.nome_usuario, Dispositivo.nome
        Order by Clima.datatemp, Usuario.nome_usuario  desc
        ) as energia
Group by data, nome_usuario, nome_dispositivo
Order by potencia_gasta asc



%spark

val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val energia_df = sqlContext.sql("select data, nome_usuario, nome_dispositivo, avg(potencia_gasta) as potencia_gasta, avg(mintemp) as mintemp, avg(maxtemp) as maxtemp, avg(avgtemp) as avgtemp, avg(totalprecip_mm) as totalprecip_mm from (select Clima.datatemp as data, Usuario.nome_usuario, Dispositivo.nome as nome_dispositivo, avg(Energia.potencia) as potencia_gasta, avg(Clima.mintemp) as mintemp, avg(Clima.maxtemp) as maxtemp, avg(Clima.avgtemp) as avgtemp, avg(Clima.totalprecip_mm) as totalprecip_mm from Zeus.Energia left outer join Zeus.Usuario on Energia.user_id = Usuario.id left outer join Zeus.Dispositivo on Energia.dispositivo_id = Dispositivo.id left outer join Zeus.Cidade on Dispositivo.id_cidade = Cidade.id left outer join Zeus.Clima on Cidade.id = Clima.id_cidade Group by Clima.datatemp, Usuario.nome_usuario, Dispositivo.nome Order by Clima.datatemp, Usuario.nome_usuario  desc) as energia Group by data, nome_usuario, nome_dispositivo")

energia_df.show( 5 )
//energia_df.printSchema()
//retail_df.show( 5 )



%spark
energia_df.printSchema()



%spark
val Array(training, testing) = energia_df.randomSplit(Array(0.6, 0.4))




%spark

import org.apache.spark.ml.feature.VectorAssembler
val assembler = new VectorAssembler()
  .setInputCols(Array("potencia_gasta", "mintemp", "maxtemp", "avgtemp", "totalprecip_mm"))
  .setOutputCol("featureSet")



%spark
val binaryClassifier = new Binarizer().setInputCol("potencia_gasta").setOutputCol("binaryLabel").setThreshold(75.0)


%spark
val lr = new LogisticRegression().setMaxIter(200).setRegParam(0.3).setElasticNetParam(0.7).setLabelCol("binaryLabel").setFeaturesCol("featureSet")


%spark
val pipeline = new Pipeline()
  .setStages(Array(assembler, binaryClassifier, lr))
  
  
  %spark
val model = pipeline.fit(training)



%spark
prediction.groupBy("nome_usuario").agg(max("potencia_gasta")).show



%spark
val matches = udf((A : Int, B: Int) => {
    if (A+B == 1) 0
    else 1
})

val total = prediction.count
val rightWrong = prediction.withColumn("matches", matches($"prediction", $"binaryLabel")).groupBy("matches").count.toDF
rightWrong.registerTempTable("rightWrong")
rightWrong.show




%spark
rightWrong.where($"matches"===1).select(rightWrong("count") / total* 100 ).show


